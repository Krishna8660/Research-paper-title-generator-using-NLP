{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79ca2019",
   "metadata": {},
   "source": [
    "### Research Paper Title Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df9b048a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.6.0)\n",
      "Requirement already satisfied: peft in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.15.2)\n",
      "Requirement already satisfied: accelerate in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.6.0)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.45.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.2.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from peft) (6.1.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from peft) (2.6.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: networkx in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\DELL\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets peft accelerate bitsandbytes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d01ed22",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfdb4308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677bedd7",
   "metadata": {},
   "source": [
    "### Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f392891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File content loaded!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = input(\"Enter the path to your file: \").strip().strip('\"')\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "        print(\"‚úÖ File content loaded!\")\n",
    "else:\n",
    "    print(\"‚ùå File not found. Please check the path.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b0a869",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f33bb935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded successfully!\n",
      "                      id                                              title  \\\n",
      "0           cs-9308101v1                               Dynamic Backtracking   \n",
      "1           cs-9308102v1  A Market-Oriented Programming Environment and ...   \n",
      "2           cs-9309101v1            An Empirical Analysis of Search in GSAT   \n",
      "3           cs-9311101v1  The Difficulties of Learning Logic Programs wi...   \n",
      "4           cs-9311102v1  Software Agents: Completing Patterns and Const...   \n",
      "...                  ...                                                ...   \n",
      "136233  abs-2408.08541v1         Where is the signal in tokenization space?   \n",
      "136234  abs-2408.08564v1  Collaborative Cross-modal Fusion with Large La...   \n",
      "136235  abs-2408.08624v1  RealMedQA: A pilot biomedical question answeri...   \n",
      "136236  abs-2408.08648v1  Understanding Enthymemes in Argument Maps: Bri...   \n",
      "136237  abs-2408.08651v2  Reasoning Beyond Bias: A Study on Counterfactu...   \n",
      "\n",
      "                                                 category category_code  \\\n",
      "0                                 Artificial Intelligence         cs.AI   \n",
      "1                                 Artificial Intelligence         cs.AI   \n",
      "2                                 Artificial Intelligence         cs.AI   \n",
      "3                                 Artificial Intelligence         cs.AI   \n",
      "4                                 Artificial Intelligence         cs.AI   \n",
      "...                                                   ...           ...   \n",
      "136233  Computation and Language (Natural Language Pro...         cs.CL   \n",
      "136234                              Information Retrieval         cs.IR   \n",
      "136235  Computation and Language (Natural Language Pro...         cs.CL   \n",
      "136236                            Artificial Intelligence         cs.AI   \n",
      "136237  Computation and Language (Natural Language Pro...         cs.CL   \n",
      "\n",
      "       published_date updated_date  \\\n",
      "0              8/1/93       8/1/93   \n",
      "1              8/1/93       8/1/93   \n",
      "2              9/1/93       9/1/93   \n",
      "3             11/1/93      11/1/93   \n",
      "4             11/1/93      11/1/93   \n",
      "...               ...          ...   \n",
      "136233        8/16/24      8/16/24   \n",
      "136234        8/16/24      8/16/24   \n",
      "136235        8/16/24      8/16/24   \n",
      "136236        8/16/24      8/16/24   \n",
      "136237        8/16/24       9/6/24   \n",
      "\n",
      "                                                  authors  \\\n",
      "0                                      ['M. L. Ginsberg']   \n",
      "1                                       ['M. P. Wellman']   \n",
      "2                              ['I. P. Gent', 'T. Walsh']   \n",
      "3          ['F. Bergadano', 'D. Gunetti', 'U. Trinchero']   \n",
      "4                    ['J. C. Schlimmer', 'L. A. Hermens']   \n",
      "...                                                   ...   \n",
      "136233  ['Renato Lui Geh', 'Honghua Zhang', 'Kareem Ah...   \n",
      "136234  ['Zhongzhou Liu', 'Hao Zhang', 'Kuicai Dong', ...   \n",
      "136235  ['Gregory Kell', 'Angus Roberts', 'Serge Umans...   \n",
      "136236  ['Jonathan Ben-Naim', 'Victor David', 'Anthony...   \n",
      "136237  ['Kyle Moore', 'Jesse Roberts', 'Thao Pham', '...   \n",
      "\n",
      "               first_author  \\\n",
      "0          'M. L. Ginsberg'   \n",
      "1           'M. P. Wellman'   \n",
      "2              'I. P. Gent'   \n",
      "3            'F. Bergadano'   \n",
      "4         'J. C. Schlimmer'   \n",
      "...                     ...   \n",
      "136233     'Renato Lui Geh'   \n",
      "136234      'Zhongzhou Liu'   \n",
      "136235       'Gregory Kell'   \n",
      "136236  'Jonathan Ben-Naim'   \n",
      "136237         'Kyle Moore'   \n",
      "\n",
      "                                                  summary  summary_word_count  \n",
      "0       Because of their occasional need to return to ...                  79  \n",
      "1       Market price systems constitute a well-underst...                 119  \n",
      "2       We describe an extensive study of search in GS...                 167  \n",
      "3       As real logic programmers normally use cut (!)...                 174  \n",
      "4       To support the goal of allowing users to recor...                 187  \n",
      "...                                                   ...                 ...  \n",
      "136233  Large Language Models (LLMs) are typically shi...                 170  \n",
      "136234  Despite the success of conventional collaborat...                 157  \n",
      "136235  Clinical question answering systems have the p...                 153  \n",
      "136236  Argument mining is natural language processing...                 194  \n",
      "136237  Language models are known to absorb biases fro...                 156  \n",
      "\n",
      "[136238 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace with your uploaded file's path\n",
    "file_path = r\"C:\\Users\\DELL\\Downloads\\archive (4)\\arXiv_scientific dataset.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(file_path)\n",
    "print(\"‚úÖ Dataset loaded successfully!\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748dbc46",
   "metadata": {},
   "source": [
    "###  Rename columns and remove missing/duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "912d504b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Because of their occasional need to return to ...</td>\n",
       "      <td>Dynamic Backtracking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Market price systems constitute a well-underst...</td>\n",
       "      <td>A Market-Oriented Programming Environment and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We describe an extensive study of search in GS...</td>\n",
       "      <td>An Empirical Analysis of Search in GSAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As real logic programmers normally use cut (!)...</td>\n",
       "      <td>The Difficulties of Learning Logic Programs wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To support the goal of allowing users to recor...</td>\n",
       "      <td>Software Agents: Completing Patterns and Const...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136233</th>\n",
       "      <td>Large Language Models (LLMs) are typically shi...</td>\n",
       "      <td>Where is the signal in tokenization space?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136234</th>\n",
       "      <td>Despite the success of conventional collaborat...</td>\n",
       "      <td>Collaborative Cross-modal Fusion with Large La...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136235</th>\n",
       "      <td>Clinical question answering systems have the p...</td>\n",
       "      <td>RealMedQA: A pilot biomedical question answeri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136236</th>\n",
       "      <td>Argument mining is natural language processing...</td>\n",
       "      <td>Understanding Enthymemes in Argument Maps: Bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136237</th>\n",
       "      <td>Language models are known to absorb biases fro...</td>\n",
       "      <td>Reasoning Beyond Bias: A Study on Counterfactu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136217 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 abstract  \\\n",
       "0       Because of their occasional need to return to ...   \n",
       "1       Market price systems constitute a well-underst...   \n",
       "2       We describe an extensive study of search in GS...   \n",
       "3       As real logic programmers normally use cut (!)...   \n",
       "4       To support the goal of allowing users to recor...   \n",
       "...                                                   ...   \n",
       "136233  Large Language Models (LLMs) are typically shi...   \n",
       "136234  Despite the success of conventional collaborat...   \n",
       "136235  Clinical question answering systems have the p...   \n",
       "136236  Argument mining is natural language processing...   \n",
       "136237  Language models are known to absorb biases fro...   \n",
       "\n",
       "                                                    title  \n",
       "0                                    Dynamic Backtracking  \n",
       "1       A Market-Oriented Programming Environment and ...  \n",
       "2                 An Empirical Analysis of Search in GSAT  \n",
       "3       The Difficulties of Learning Logic Programs wi...  \n",
       "4       Software Agents: Completing Patterns and Const...  \n",
       "...                                                   ...  \n",
       "136233         Where is the signal in tokenization space?  \n",
       "136234  Collaborative Cross-modal Fusion with Large La...  \n",
       "136235  RealMedQA: A pilot biomedical question answeri...  \n",
       "136236  Understanding Enthymemes in Argument Maps: Bri...  \n",
       "136237  Reasoning Beyond Bias: A Study on Counterfactu...  \n",
       "\n",
       "[136217 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename(columns={'summary': 'abstract', 'summary_word_count': 'abstract_count'})\n",
    "df = df[['abstract', 'title']].dropna().drop_duplicates()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b342c2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Because of their occasional need to return to ...</td>\n",
       "      <td>Dynamic Backtracking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Market price systems constitute a well-underst...</td>\n",
       "      <td>A Market-Oriented Programming Environment and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We describe an extensive study of search in GS...</td>\n",
       "      <td>An Empirical Analysis of Search in GSAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As real logic programmers normally use cut (!)...</td>\n",
       "      <td>The Difficulties of Learning Logic Programs wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To support the goal of allowing users to recor...</td>\n",
       "      <td>Software Agents: Completing Patterns and Const...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>A fuzzy mnesor space is a semimodule over the ...</td>\n",
       "      <td>Fuzzy Mnesors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>We apply proof-theoretic techniques in answer ...</td>\n",
       "      <td>An Application of Proof-Theory in Answer Set P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>We show that some common and important global ...</td>\n",
       "      <td>Decompositions of All Different, Global Cardin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>To model combinatorial decision problems invol...</td>\n",
       "      <td>Scenario-based Stochastic Constraint Programming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Many real life optimization problems contain b...</td>\n",
       "      <td>Reasoning about soft constraints and condition...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              abstract  \\\n",
       "0    Because of their occasional need to return to ...   \n",
       "1    Market price systems constitute a well-underst...   \n",
       "2    We describe an extensive study of search in GS...   \n",
       "3    As real logic programmers normally use cut (!)...   \n",
       "4    To support the goal of allowing users to recor...   \n",
       "..                                                 ...   \n",
       "495  A fuzzy mnesor space is a semimodule over the ...   \n",
       "496  We apply proof-theoretic techniques in answer ...   \n",
       "497  We show that some common and important global ...   \n",
       "498  To model combinatorial decision problems invol...   \n",
       "499  Many real life optimization problems contain b...   \n",
       "\n",
       "                                                 title  \n",
       "0                                 Dynamic Backtracking  \n",
       "1    A Market-Oriented Programming Environment and ...  \n",
       "2              An Empirical Analysis of Search in GSAT  \n",
       "3    The Difficulties of Learning Logic Programs wi...  \n",
       "4    Software Agents: Completing Patterns and Const...  \n",
       "..                                                 ...  \n",
       "495                                      Fuzzy Mnesors  \n",
       "496  An Application of Proof-Theory in Answer Set P...  \n",
       "497  Decompositions of All Different, Global Cardin...  \n",
       "498   Scenario-based Stochastic Constraint Programming  \n",
       "499  Reasoning about soft constraints and condition...  \n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.head(500)  \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb403b2",
   "metadata": {},
   "source": [
    "### Tokenize Using GPT-2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "507d3626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT2 doesn't use pad_token, so set eos_token as pad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb51d9",
   "metadata": {},
   "source": [
    "### Preprocess and Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5510e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def preprocess_and_tokenize(df, tokenizer, max_length=512):\n",
    "    tokenized_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        abstract = row['abstract']\n",
    "        title = row['title']\n",
    "\n",
    "        prompt = f\"### Scientific Abstract:\\n{abstract}\\n### Predicted Title:\"\n",
    "        full_input = f\"{prompt} {title}\"\n",
    "\n",
    "        encoding = tokenizer(\n",
    "            full_input,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoding[\"input_ids\"]\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # Ignore loss for the prompt part\n",
    "        prompt_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        prompt_len = len(prompt_ids)\n",
    "        labels[0, :prompt_len] = -100\n",
    "        encoding[\"labels\"] = labels\n",
    "\n",
    "        tokenized_list.append(encoding)\n",
    "\n",
    "    return tokenized_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dba428c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the dataset\n",
    "tokenized_inputs = preprocess_and_tokenize(df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ebc073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for training\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "class TitleDataset(TorchDataset):\n",
    "    def __init__(self, tokenized_list):\n",
    "        self.tokenized_list = tokenized_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val.squeeze() for key, val in self.tokenized_list[idx].items()}\n",
    "\n",
    "train_dataset = TitleDataset(tokenized_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeca3c41",
   "metadata": {},
   "source": [
    "### Load GPT-2 Model and Apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f863fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\peft\\tuners\\lora\\layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 with LoRA\n",
    "from transformers import GPT2LMHeadModel\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Better LoRA coverage\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\", \"mlp.c_fc\", \"mlp.c_proj\", \"attn.c_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b4faf7",
   "metadata": {},
   "source": [
    "-LoRA adds trainable adapters to GPT-2\n",
    "\n",
    "-Efficient fine-tuning without changing all model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9bdb4c",
   "metadata": {},
   "source": [
    "üí° LoRA Solution: Fine-tune Efficiently LoRA stands for Low-Rank Adaptation of Large Language Models. It freezes the original model weights and adds a few small trainable matrices that:\n",
    "\n",
    "-Learn task-specific knowledge.\n",
    "\n",
    "-Significantly reduce the number of trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc2ce58",
   "metadata": {},
   "source": [
    "### Training Configuration & Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b10b97f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11320\\2408475910.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 1:28:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.148700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.902600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.037400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.765800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.912800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.932900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.338100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.159400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.163700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.504700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.982800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.057300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4.106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.204800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>4.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>4.081100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>4.175800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>4.291300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.975100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>4.147600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>4.078900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>4.303200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>4.253500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>4.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>4.242300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>4.406400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>4.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.992100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>4.122800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>4.330200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>4.017700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>4.147600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>4.494400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>4.060200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>4.668600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>4.472100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>4.071200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.906100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3.947900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>4.166500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>4.341300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.914000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.779600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.838600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>4.067100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.873200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>4.434500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.987900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>3.943100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3.980400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>4.117100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>4.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>4.250600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>4.077400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>4.069800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>3.447600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>4.110200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.054400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>4.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>3.920800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>3.914000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>4.154800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>4.048500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>3.924200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>4.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>4.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>4.177800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.794000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>3.742600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>3.936500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>4.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>4.011800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>3.796200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>3.823800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>3.930800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>3.916500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>3.668300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>4.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>3.767100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>3.744100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>3.751000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>3.847500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>3.785300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>3.762400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>3.707400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>3.703600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>4.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>4.080200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>3.682700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>3.445100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>3.480200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>4.076200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>3.845500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>3.953900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>4.089200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>3.835800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.880200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>3.763400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>3.840700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>3.774500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>3.897700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>3.551300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>3.820200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>3.874200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>3.576600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>3.696100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.929300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>3.815100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>3.581600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>4.055600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>3.767200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>3.675200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>3.465700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>3.590400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>3.658200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>3.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.376900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>3.751100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>3.493500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>3.658900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>3.738800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>3.932500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>3.429600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>3.710100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>3.752700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>3.468700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>3.714800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>3.835000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>3.639200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>3.736300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>3.666500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>3.696100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>3.662200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>3.193600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>3.570700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>3.490300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.464800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>3.764700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>3.808000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>3.530100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>3.848200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>4.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>3.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>3.436400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>3.543700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>3.402900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.461900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>3.377900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>3.455000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>3.699700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>3.598900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>3.666900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>3.697300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>3.708800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>3.466400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>3.442200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.476200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>3.613700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>3.809500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>3.277300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>3.566000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>3.608300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>3.545400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>3.695200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>3.503200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>3.622900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>3.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>3.490300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>3.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>3.497200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>3.534300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>3.583500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>3.362900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>3.590600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>3.704500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>3.559300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.796000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>3.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>3.512600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>3.502500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>3.664300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>3.708800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>3.624800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>3.318900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>3.708800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>3.764000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.622600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>3.704000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>3.332300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>3.501900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>3.584300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>3.642400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>3.597300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>3.448300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>3.562800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>3.676900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.548200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>3.580200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>3.368500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>3.530500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>3.323400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>3.585800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>3.739400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>3.585400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>3.489600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>3.617900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>3.409100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>3.448700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>3.465400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>3.602500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>3.553700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>3.641500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>3.550100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>3.378300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>3.676800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>3.419900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>3.163300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>3.404600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>3.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>3.412600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>3.415100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>3.583500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>3.417300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>3.527500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>3.372600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>3.453000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>3.532700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>3.505300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>3.187100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>3.718600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>3.271300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>3.512000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>3.642600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>3.437700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>3.833000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>3.548500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>3.295500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>3.537900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>3.424700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>3.456300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>3.707800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>3.282700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>3.533600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>3.542400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>3.263900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>3.212400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.700200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>3.208400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>3.321600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>3.415900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>3.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>3.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>3.438500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>3.267700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>3.236000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>3.497600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>3.403000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>3.206500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>3.534300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>3.640900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>3.282400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>3.830800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>3.213700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>3.549500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>3.553500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>3.492400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>3.315500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>3.728300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>3.942900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>3.535100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>3.541500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>3.599100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>3.432500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>3.391300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>3.517800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>3.572100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>3.540600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>3.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>3.378500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>3.487200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>3.223100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>3.416100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>3.318100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>3.381400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>3.546700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>3.585100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>3.468900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>3.539700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>3.519700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>3.983500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>3.661000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>3.268100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>3.650100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>3.577100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>3.417300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>3.476100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.468400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>3.602600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>3.313600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>3.192500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>3.285700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>3.487800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>3.192500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>3.342700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>3.761800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>3.457900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>3.460100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>3.518200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>3.385300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>3.413600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>3.846600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>3.362100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>3.227400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>3.538200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>3.410800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>3.659800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>3.600900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>3.681500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>3.501600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>3.545800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>3.701800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>3.507700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>3.571200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>3.429800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>3.527900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>3.359200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>3.447300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>3.498700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>3.544800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>3.325800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>3.395400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>3.501900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>3.422600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>3.743500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>3.167900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339</td>\n",
       "      <td>3.332000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>3.333400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>3.138700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>3.380100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>3.421200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>3.464100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>3.574200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>346</td>\n",
       "      <td>3.660200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>3.575600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>3.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>3.676500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.471300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>3.711500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>3.557700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353</td>\n",
       "      <td>3.413700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>354</td>\n",
       "      <td>3.472300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>3.536500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>3.377100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>3.280400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358</td>\n",
       "      <td>3.932400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359</td>\n",
       "      <td>3.531100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>3.497200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>361</td>\n",
       "      <td>3.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362</td>\n",
       "      <td>3.585300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>363</td>\n",
       "      <td>3.738300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364</td>\n",
       "      <td>3.693600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>3.347600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>366</td>\n",
       "      <td>3.641600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367</td>\n",
       "      <td>3.200200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>3.209500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369</td>\n",
       "      <td>3.451700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>3.762000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>3.963800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>3.449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>373</td>\n",
       "      <td>3.457200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374</td>\n",
       "      <td>3.419300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>3.275200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=3.666485568364461, metrics={'train_runtime': 5338.1499, 'train_samples_per_second': 0.281, 'train_steps_per_second': 0.07, 'total_flos': 394655956992000.0, 'train_loss': 3.666485568364461, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer setup\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_lora_title_model\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    logging_steps=1,\n",
    "    save_total_limit=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=None\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c9a7c",
   "metadata": {},
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "004dcbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('D:\\\\nlp project\\\\gpt2_lora_title_model\\\\tokenizer_config.json',\n",
       " 'D:\\\\nlp project\\\\gpt2_lora_title_model\\\\special_tokens_map.json',\n",
       " 'D:\\\\nlp project\\\\gpt2_lora_title_model\\\\vocab.json',\n",
       " 'D:\\\\nlp project\\\\gpt2_lora_title_model\\\\merges.txt',\n",
       " 'D:\\\\nlp project\\\\gpt2_lora_title_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(r\"D:\\nlp project\\gpt2_lora_title_model\")\n",
    "tokenizer.save_pretrained(r\"D:\\nlp project\\gpt2_lora_title_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415e6ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df4fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT2LMHeadModel.from_pretrained(r\"D:\\nlp project\\gpt2_lora_title_model\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(r\"D:\\nlp project\\gpt2_lora_title_model\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Load SBERT model\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to generate title\n",
    "def generate_title_lora(abstract, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    prompt = (\n",
    "        \"Given the following abstract, generate a concise academic paper title \"\n",
    "        \"(no more than 12 words):\\n\\n\"\n",
    "        f\"{abstract}\\n\\nTitle:\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=30,  # Increased for more complete outputs\n",
    "            do_sample=False,\n",
    "            num_beams=5,\n",
    "            no_repeat_ngram_size=2,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    match = re.search(r\"Title:\\s*(.*)\", decoded, re.IGNORECASE)\n",
    "    if match:\n",
    "        title = match.group(1).strip()\n",
    "    else:\n",
    "        title = decoded.strip()\n",
    "\n",
    "    title = re.sub(r'[^\\w\\s-]', '', title).strip()\n",
    "    title_tokens = title.split()\n",
    "    if len(title_tokens) > 15:\n",
    "        title = ' '.join(title_tokens[:15])\n",
    "\n",
    "    return title\n",
    "\n",
    "# Semantic similarity using SBERT\n",
    "def semantic_similarity_sbert(title1, title2):\n",
    "    embeddings = sbert_model.encode([title1, title2], convert_to_tensor=True)\n",
    "    similarity = util.pytorch_cos_sim(embeddings[0], embeddings[1])\n",
    "    return similarity.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8436a6",
   "metadata": {},
   "source": [
    "* semantic_similarity_sbert measures **semantic similarity** (meaning) between two sentences.\n",
    "* Sentence Bidirectional Encoder Representations from Transformers\n",
    "* It compares the **meaning** of a **generated title** with a **true title**.\n",
    "* Focuses on **meaningful similarity**, not just word overlap.\n",
    "* Uses **sentence embeddings** and computes **cosine similarity**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5efe1d",
   "metadata": {},
   "source": [
    "Score range: 0 (completely different) to 1 (identical in meaning)\n",
    "\n",
    "0.0 ‚Äì 0.3: Little to no semantic similarity\n",
    "\n",
    "0.3 ‚Äì 0.6: Somewhat similar, but not strongly aligned\n",
    "\n",
    "0.6 ‚Äì 0.8: Moderate to strong semantic similarity\n",
    "\n",
    "0.8 ‚Äì 1.0: Highly or near-identical meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff51f9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Index: 0\n",
      "üìù Abstract: Because of their occasional need to return to shallow points in a search\n",
      "tree, existing backtracking methods can sometimes erase meaningful progress\n",
      "toward solving a search problem. In this paper, we present a method by which\n",
      "backtrack points can be moved deeper in the search space, thereby avoiding this\n",
      "difficulty. The technique developed is a variant of dependency-directed\n",
      "backtracking that uses only polynomial space while still providing useful\n",
      "control information and retaining the completeness guarantees provided by\n",
      "earlier approaches.\n",
      "‚úÖ Original Title: Dynamic Backtracking\n",
      "üéØ Generated Title: A Dependent-Directed Backtracking Method for Searching Deep Points in Search Trees\n",
      "üîç Semantic Similarity Score: 0.5740\n",
      "\n",
      "üìÑ Index: 1\n",
      "üìù Abstract: Market price systems constitute a well-understood class of mechanisms that\n",
      "under certain conditions provide effective decentralization of decision making\n",
      "with minimal communication overhead. In a market-oriented programming approach\n",
      "to distributed problem solving, we derive the activities and resource\n",
      "allocations for a set of computational agents by computing the competitive\n",
      "equilibrium of an artificial economy. WALRAS provides basic constructs for\n",
      "defining computational market structures, and protocols for deriving their\n",
      "corresponding price equilibria. In a particular realization of this approach\n",
      "for a form of multicommodity flow problem, we see that careful construction of\n",
      "the decision process according to economic principles can lead to efficient\n",
      "distributed resource allocation, and that the behavior of the system can be\n",
      "meaningfully analyzed in economic terms.\n",
      "‚úÖ Original Title: A Market-Oriented Programming Environment and its Application to\n",
      "  Distributed Multicommodity Flow Problems\n",
      "üéØ Generated Title: A Decentralized Decision-Making System for Distributed Problem Solving\n",
      "üîç Semantic Similarity Score: 0.5100\n",
      "\n",
      "üìÑ Index: 2\n",
      "üìù Abstract: We describe an extensive study of search in GSAT, an approximation procedure\n",
      "for propositional satisfiability. GSAT performs greedy hill-climbing on the\n",
      "number of satisfied clauses in a truth assignment. Our experiments provide a\n",
      "more complete picture of GSAT's search than previous accounts. We describe in\n",
      "detail the two phases of search: rapid hill-climbing followed by a long plateau\n",
      "search. We demonstrate that when applied to randomly generated 3SAT problems,\n",
      "there is a very simple scaling with problem size for both the mean number of\n",
      "satisfied clauses and the mean branching rate. Our results allow us to make\n",
      "detailed numerical conjectures about the length of the hill-climbing phase, the\n",
      "average gradient of this phase, and to conjecture that both the average score\n",
      "and average branching rate decay exponentially during plateau search. We end by\n",
      "showing how these results can be used to direct future theoretical analysis.\n",
      "This work provides a case study of how computer experiments can be used to\n",
      "improve understanding of the theoretical properties of algorithms.\n",
      "‚úÖ Original Title: An Empirical Analysis of Search in GSAT\n",
      "üéØ Generated Title: The Search for Satisfied Clauses in SAT Problems\n",
      "üîç Semantic Similarity Score: 0.3342\n",
      "\n",
      "üìÑ Index: 3\n",
      "üìù Abstract: As real logic programmers normally use cut (!), an effective learning\n",
      "procedure for logic programs should be able to deal with it. Because the cut\n",
      "predicate has only a procedural meaning, clauses containing cut cannot be\n",
      "learned using an extensional evaluation method, as is done in most learning\n",
      "systems. On the other hand, searching a space of possible programs (instead of\n",
      "a space of independent clauses) is unfeasible. An alternative solution is to\n",
      "generate first a candidate base program which covers the positive examples, and\n",
      "then make it consistent by inserting cut where appropriate. The problem of\n",
      "learning programs with cut has not been investigated before and this seems to\n",
      "be a natural and reasonable approach. We generalize this scheme and investigate\n",
      "the difficulties that arise. Some of the major shortcomings are actually\n",
      "caused, in general, by the need for intensional evaluation. As a conclusion,\n",
      "the analysis of this paper suggests, on precise and technical grounds, that\n",
      "learning cut is difficult, and current induction techniques should probably be\n",
      "restricted to purely declarative logic languages.\n",
      "‚úÖ Original Title: The Difficulties of Learning Logic Programs with Cut\n",
      "üéØ Generated Title: An Extensional Evaluation Approach for Logic Programs with Cut Predicates\n",
      "üîç Semantic Similarity Score: 0.7356\n",
      "\n",
      "üìÑ Index: 4\n",
      "üìù Abstract: To support the goal of allowing users to record and retrieve information,\n",
      "this paper describes an interactive note-taking system for pen-based computers\n",
      "with two distinctive features. First, it actively predicts what the user is\n",
      "going to write. Second, it automatically constructs a custom, button-box user\n",
      "interface on request. The system is an example of a learning-apprentice\n",
      "software- agent. A machine learning component characterizes the syntax and\n",
      "semantics of the user's information. A performance system uses this learned\n",
      "information to generate completion strings and construct a user interface.\n",
      "Description of Online Appendix: People like to record information. Doing this\n",
      "on paper is initially efficient, but lacks flexibility. Recording information\n",
      "on a computer is less efficient but more powerful. In our new note taking\n",
      "softwre, the user records information directly on a computer. Behind the\n",
      "interface, an agent acts for the user. To help, it provides defaults and\n",
      "constructs a custom user interface. The demonstration is a QuickTime movie of\n",
      "the note taking agent in action. The file is a binhexed self-extracting\n",
      "archive. Macintosh utilities for binhex are available from\n",
      "mac.archive.umich.edu. QuickTime is available from ftp.apple.com in the\n",
      "dts/mac/sys.soft/quicktime.\n",
      "‚úÖ Original Title: Software Agents: Completing Patterns and Constructing User Interfaces\n",
      "üéØ Generated Title: A Learning-Apprentice Software-Agent for Pen-Based Computers with Two Different\n",
      "üîç Semantic Similarity Score: 0.4862\n",
      "\n",
      "üìÑ Index: 5\n",
      "üìù Abstract: Terminological knowledge representation systems (TKRSs) are tools for\n",
      "designing and using knowledge bases that make use of terminological languages\n",
      "(or concept languages). We analyze from a theoretical point of view a TKRS\n",
      "whose capabilities go beyond the ones of presently available TKRSs. The new\n",
      "features studied, often required in practical applications, can be summarized\n",
      "in three main points. First, we consider a highly expressive terminological\n",
      "language, called ALCNR, including general complements of concepts, number\n",
      "restrictions and role conjunction. Second, we allow to express inclusion\n",
      "statements between general concepts, and terminological cycles as a particular\n",
      "case. Third, we prove the decidability of a number of desirable TKRS-deduction\n",
      "services (like satisfiability, subsumption and instance checking) through a\n",
      "sound, complete and terminating calculus for reasoning in ALCNR-knowledge\n",
      "bases. Our calculus extends the general technique of constraint systems. As a\n",
      "byproduct of the proof, we get also the result that inclusion statements in\n",
      "ALCNR can be simulated by terminological cycles, if descriptive semantics is\n",
      "adopted.\n",
      "‚úÖ Original Title: Decidable Reasoning in Terminological Knowledge Representation Systems\n",
      "üéØ Generated Title: Abstract Terminological Knowledge Representation Systems for Designing Knowledge Basis\n",
      "üîç Semantic Similarity Score: 0.7347\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for idx in range(6):\n",
    "    abstract = df.iloc[idx]['abstract']\n",
    "    original_title = df.iloc[idx]['title']\n",
    "    generated_title = generate_title_lora(abstract, model, tokenizer, device)\n",
    "    similarity = semantic_similarity_sbert(original_title, generated_title)\n",
    "\n",
    "    print(f\"\\nüìÑ Index: {idx}\")\n",
    "    print(f\"üìù Abstract: {abstract}\")\n",
    "    print(f\"‚úÖ Original Title: {original_title}\")\n",
    "    print(f\"üéØ Generated Title: {generated_title}\")\n",
    "    print(f\"üîç Semantic Similarity Score: {similarity:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        'Index': idx,\n",
    "        'Original Title': original_title,\n",
    "        'Generated Title': generated_title,\n",
    "        'Similarity Score': similarity\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "797f7120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create DataFrame and plot bar graph\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(results_df['Index'], results_df['Similarity Score'], color='skyblue', edgecolor='black')\n",
    "plt.title('Semantic Similarity: Original vs Generated Titles (Index 0‚Äì5)')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('SBERT Similarity Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(results_df['Index'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ade6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad570fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f4e908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109f9ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70be74f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56b11fee",
   "metadata": {},
   "source": [
    "| Step | Title                           | Summary                                                                       |\n",
    "| ---- | ------------------------------- | ----------------------------------------------------------------------------- |\n",
    "| 1    | **Install Libraries**           | Installs: `transformers`, `datasets`, `peft`, `accelerate`, `bitsandbytes`.   |\n",
    "| 2    | **Import Libraries**            | Imports Pandas, PyTorch, Transformers (GPT-2), PEFT (for LoRA).               |\n",
    "| 3    | **Upload Dataset**              | Uses `os` and direct file path to access the CSV dataset.                     |\n",
    "| 4    | **Load Dataset**                | Reads the CSV file using `pandas.read_csv()`.                                 |\n",
    "| 5    | **Clean Dataset**               | Renames the `summary` column to `abstract`, removes null or duplicate rows.   |\n",
    "| 6    | **Tokenize Dataset**            | Loads the GPT-2 tokenizer with padding and truncation for abstracts.          |\n",
    "| 7    | **Preprocess for Model Input**  | Converts the DataFrame into a HuggingFace `Dataset` and tokenizes it.         |\n",
    "| 8    | **Load GPT-2 Model with LoRA**  | Loads the GPT-2 model and applies Low-Rank Adaptation (LoRA) for fine-tuning. |\n",
    "| 9    | **Setup Training Arguments**    | Defines batch size, epochs, and logging strategy for training.                |\n",
    "| 10   | **Train Model**                 | Uses the `Trainer` class to fine-tune GPT-2 on abstract ‚Üí title mapping.      |\n",
    "| 11   | **Generate Title for Abstract** | Loads the fine-tuned model and generates a title for a new abstract.          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecd9f3a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
